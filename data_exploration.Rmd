---
title: "kaggle-naukri-jobdb"
author: "minghao"
date: "October 21, 2017"
output: html_document
---

# Intro

The dataset is given on the context that this is a pre-crawled dataset, taken as subset of a bigger dataset that was created by extracting data from Naukri.com, a leading job board.

sadly it is not known how the data were subseted, whether analysising this data would give a breif overview with regards to job market over a period of time or certain industry. 


# libraries
```{r}
require(tidyverse)
require(data.table)
require(lubridate)
require(stringr)
require(ggvis)

require(ggplot2)
require(forcats)
```

```{r}
# setwd(dir="D:\\Users\\gdrtmh\\Desktop\\Kaggle\\Jobs on Naukri")
inputdt <- as.data.table(read.csv(file="naukri_com-job_sample.csv", strip.white = TRUE, 
                             na.strings=c("","NA","Not Mentioned","Not Disclosed by Recruiter"), 
                             stringsAsFactors = FALSE))

# setting "Not mentioned" as NA for experience etc.
# setting "Not Disclosed by Recruiter" that is the norm for NA in payrate

```

# Data Exploration

```{r}
glimpse(inputdt)
```

## site_name
```{r}
summary(as.factor(inputdt$site_name))
inputdt$site_name <- NULL
```
Well, it seems like the site name is pretty filled with either naukri or NA data. This column could probably be removed without influencing the subsequent data analysis.

## uniq_id

```{r}
uniqueN(inputdt$uniq_id)
```

This Would be useful as an unique identifier for individual job listing.

## jobid

```{r}
uniqueN(inputdt, by=c("jobid"))
```

It is not exactly clear how "jobid" is structed, perhaps it accounts of relisting of jobs? 

Although, since the number of unique jobid is only differing from uniq_id by 90, if the purpose where to depict the relisting of respective jobs, it probably did not do its job very well.


## Payrate

```{r}
# Seeing that payrate listing would corresponds to format of "1,50,000 - 2,50,000 P.A"
# I would just use gsub to remove strings after P.A
inputdt$payrate.M<-gsub( " P.A.*$", "", inputdt$payrate)
# this is essential to remove numerical characters after the P.A string
# this also removed certain listed incentive/commission information in payrate

# stripping all the non numerical chr from the payrate
inputdt$payrate.M<-gsub("[^0-9-]", "", inputdt$payrate.M)
# inputdt$m.payrate<-gsub("[^0-9,-,-]", "", inputdt$m.payrate)

# Outliers
# Example of some payrate that is not extracated properly ( with 2 "-" etc after regexp treatment), 
# seeing there is no easy solution and that there is only ~200 of them, them will be ignored for now
inputdt[lengths(strsplit(inputdt$payrate.M, "-"))>2, list(payrate,payrate.M),]
```

COuld potentially improve the regular expresssion by extracting only numbers between the dashes, but that would require more work with regeexp, going to put it off for now.

Splitting the payrate column while conditionally ignoring those that have more than dash ("-") symbol.

```{r}

# Identifying the appropriate number of splits for payrate with str "-"
splits <- max(lengths(strsplit(inputdt$payrate.M, "-")))

# Spliting the payrate into m.payrate1 and m.payrate2, Ignoring those column with additional splits
mdt <- inputdt[lengths(strsplit(inputdt$payrate.M, "-"))<=2,
               paste0("m.payrate", 1:2) := tstrsplit(payrate.M, "-", fixed=TRUE)][] 

# Changing the column into numerics
mdt$m.payrate1<- as.numeric(mdt$m.payrate1)
mdt$m.payrate2<- as.numeric(mdt$m.payrate2)

# Creating a column that indicate the mean of the payrate, would be easier on plotting etc
mdt[, m.payrate.Mean := rowMeans(.SD), by = uniq_id, .SDcols = c("m.payrate1", "m.payrate2")]
```

The mean value and median value of the payrate should almost be equivalent given that in this case, they are both determined from the listed upper and lower window of payrate. 

```{r}
# Checking if the values in experience1 are always lower than experience2, 
# such that experience1 can be considered as a lower limit for the job, vice versa for experience2
# summary(mdt$m.payrate1>mdt$m.payrate2)
#    Mode   FALSE    TRUE    NA's 
# logical       2    4743   17255 

# listing the data with m.payrate1 > m.payrate2
mdt[m.payrate1>m.payrate2,list(payrate,payrate.M,m.payrate1,m.payrate2)]
```

In both of these cases, the payrate is not well formated and extracted. There is probably some bad extraction in the extracting of payrate that is not depicted here as well as they may be hidden behind m.payrate2 > m.payrate1.

Removing these cases (where m.payrate1>m.payrate2) by setting them to NA.


```{r}
mdt[(m.payrate1>m.payrate2),`:=`(payrate.M= NA,
                                 m.payrate1= NA,
                                 m.payrate2= NA,
                                 m.payrate.Mean= NA)]


# The majority of the records has no payrate listed.
# summary(mdt$m.payrate.Mean)
#      Min.   1st Qu.    Median      Mean   3rd Qu.      Max.      NA's 
# 9.000e+00 2.500e+05 3.750e+05 3.672e+09 8.000e+05 1.740e+13     17255 
# The maximum figure also points to likely error in extracation

# after attempting a few threshold of cut, I find 9e+6 to be most suitable, listed below are a table depicted payrate after this threshold.
mdt[m.payrate.Mean>9e+6] %>%
  .[,list(payrate, m.payrate.Mean)]

```

In fact, it almost seems like the posting of payrate = "Pay Band: PB3 (Rs.15600-39100) with Academic Grade Pay of Rs.8,000/p. m." are just reposting seeing about 80 listing of jobs are having identifical payrate.

```{r}
# for an easy way out, it seems just removing all extracted payrate data for these badly extracted payrate will do. ie:
mdt[m.payrate.Mean>9e+6,`:=`(payrate.M= NA,
                             m.payrate1= NA,
                             m.payrate2= NA,
                             m.payrate.Mean= NA)]


#total count of valid payrate records
N_valid_payrate<-sum(!is.na(mdt$m.payrate.Mean))

payrate_record_percentage <- sum(!is.na(mdt$m.payrate.Mean))/sum((mdt$m.payrate.Mean), na.rm=T)
```



Sadly, there is only `payrate_record_percentage` of the job listing that has its payrate listed and extracted so fr, that would be a mere `N_valid_payrate` records.

```{r}
# all_values <- function(x) {
#   if(is.null(x)) return(NULL)
#   paste0(names(x), ": ", format(x), collapse = "<br />")
# }

mdt[!is.na(m.payrate.Mean),][m.payrate.Mean<9e+6] %>%
  melt(.,  measure.vars = patterns("^m.payrate")) %>%
  ggvis(~value, fill = ~variable) %>%
  group_by(variable) %>%
  layer_densities() %>%
  add_axis("x", title = "Payrate Per Annum") %>%
  add_axis("y", title = "Density")



# Oddly I couldn't find documented (and not dirty) approach in inserting a plot title with ggvis. I will just leave it as it is for now.

```

Would be interesting to see the how the payrate woudl vary per industry/skill/location in a later section. Though, if we narrow down our windowing.


```{r}
mdt[!is.na(m.payrate.Mean),][m.payrate.Mean<2e+6] %>%
  melt(.,  measure.vars = patterns("^m.payrate")) %>%
  ggvis(~value, fill = ~variable) %>%
  group_by(variable) %>%
  layer_densities() %>%
  add_axis("x", title = "Payrate Per Annum") %>%
  add_axis("y", title = "Density")

```

Well it seems that for the payrate of the listed jobs, the median of listed salary is just about ~300k repee PA ( since the jobboard seems to be based in india).

## skills

```{r}
#head of records in skill
head(mdt$skills)

# unique(inputdt$skills)
```
listing of top 10 skill in demand

```{r}

mdt[!is.na(skills),.N,by=list(skills)] %>%
  .[,.SD[order(-N)]] %>%
  .[, head(.SD, 10),]%>%
  .[,value:=fct_inorder(factor(skills))] %>% # fct_infreq from "forcats" to reoder the factor in x axis
  ggvis(x=~skills, y=~N) %>%
  layer_bars(fillOpacity := 0.1)%>%
  add_axis("x", properties = axis_props(
    labels = list(angle = 45, align = "left", fontSize = 10)
  ))
```

This shows that the most demanding skills are IT Software - Application Programming, followed by Sales and ITES.

## industry
```{r}

# Checking whether every val  id entry has "yrs" in the records, 
# summary(!is.na(mdt$industry))
#    Mode   FALSE    TRUE    NA's 
# logical       5   21995       0 
# Majority of the job listing did listed valid industry

head(mdt$industry)


```
It seems most job listing are determined to be in a cross of several industry

```{r}
# splitting the mdt$industry with "/" into industryS1, industryS2 etc
splits <- max(lengths(strsplit(mdt$industry, "/")))
mdt <- mdt[,paste0("industryS", 1:splits) := tstrsplit(industry, "/", fixed=TRUE)][]
```

### Most Popular industry

```{r}
# note that this  would also duplicate job listing with multiple industry for certain records, since some of the job listing are listed to have multiple industry 
mdt.melt.industry <- melt(mdt,  measure.vars = patterns("^industryS")) 

mdt.melt.industry$value <- (trimws(mdt.melt.industry$value))

mdt.melt.industry  %>%
  .[!is.na(value),.N,by=list(value)]%>%
  .[,.SD[order(-N)]] %>%
  .[, head(.SD, 10),]%>%
  .[,value:=fct_inorder(factor(value))] %>% # fct_infreq from "forcats" to reoder the factor in x axis
  ggvis(x=~value, y=~N) %>% 
  layer_bars(fillOpacity := 0.1)%>%
  add_axis("x", title = "Industry",properties = axis_props(
    labels = list(angle = 45, align = "left", fontSize = 10)
  )) %>%
  add_axis("y", title = "Number of posting")



```

###Least popular Industry

```{r}

mdt.melt.industry[!is.na(value),.N,by=list(value)]  %>%
  .[,.SD[order(-N)]] %>%
  .[, tail(.SD, 10),]%>%
  .[,value:=fct_inorder(factor(value))] %>% # fct_infreq from "forcats" to reoder the factor in x axis
  ggvis(x=~as.factor(value), y=~N) %>%
  layer_bars(fillOpacity := 0.1)%>%
  add_axis("x", title = "Industry") %>%
  add_axis("y", title = "Number of posting")



```
## Experience

```{r}
# dt[!str_detect(dt$experience, "yrs")]# It seems all the listing are in a matter of years

# one entry with  double dash "-"
# glimpse(dt[max(lengths(strsplit(dt$experience, "-")))==lengths(strsplit(dt$experience, "-"))])
mdt[max(lengths(strsplit(mdt$experience, "-")))==lengths(strsplit(mdt$experience, "-"))]$experience <- "1 - 3 yrs"

# Removing the last 3 char - "yrs" in the experience records
# nchar(mdt$experience, allowNA = TRUE)-3
mdt$experience.M <- str_sub(mdt$experience, 1,nchar(mdt$experience, allowNA = TRUE)-4)

# Splitting the experience
splits <- max(lengths(strsplit(mdt$experience.M, "-")))
mdt <- mdt[,paste0("experience", 1:splits) := tstrsplit(experience.M, "-", fixed=TRUE)][]

# Changing them to appropriate class
mdt$experience1<-as.numeric(mdt$experience1)
mdt$experience2<-as.numeric(mdt$experience2)

# Checking if the values in experience1 are always lower than experience2, 
# such that experience1 can be considered as a lower limit for the job, vice versa for experience2
# summary(mdt$experience1<=mdt$experience2)
#    Mode    TRUE    NA's 
# logical   21885     115 

# renaming the experience1 and experience2 column into more obvious form
setnames(mdt, c("experience1","experience2"), c("m.experience.L", "m.experience.U"))


mdt[!is.na(m.experience.L),]%>%
  melt(.,  measure.vars = patterns("^m.experience")) %>%
  ggvis(~value, fill = ~variable) %>%
  group_by(variable) %>%
  layer_densities(adjust = 2) %>%  #adjust parameter to smooth the wiggliness of density
  add_axis("x", title = "Number of years") %>%
  add_axis("y", title = "Density")

```

## Date Time

split and time zone
```{r}
head(mdt$postdate)

#str split the postdate based on format corresponds to "2016-05-21 19:30:00 +0000" on "+"
mdt[, c("postdate_time","postdate_timezone") := tstrsplit(postdate,"+",2)]

# summary(as.factor(mdt$postdate_timezone))
# 0000  NA's 
# 21977    23 
# it seems timezone is pretty meaningless in the data base too

# Removing timezone column
mdt$postdate_timezone <- NULL
```

formatting

```{r}
mdt$postdate_time <- parse_date_time(mdt$postdate_time, "%Y-%m-%d H:M:S")
#ignoring timezone for now, shouldn't make any difference too

min(mdt$postdate_time, na.rm =T)
max(mdt$postdate_time, na.rm =T)
```

### Number of job posting over time

```{r}


#
mdt[, timeCut := cut(as.POSIXct(postdate_time), breaks="1 month")]

mdt[!is.na(timeCut), .N, by=timeCut] %>%
  ggvis(~timeCut, ~N)%>%
  layer_bars(fillOpacity := 0.1) %>%
  add_axis("x", properties = axis_props(
    labels = list(angle = 45, align = "left", fontSize = 10)
  ))

# alternative data table method
# mdt[, .N, by=month(postdate_time)] 

# alternate dplyr solution
# mdt$uniq_id
# mdt %>% 
#   group_by(yr = year(postdate_time), mon = month(postdate_time)) %>% 
#   summarise(mn_amt = length(uniq_id))

```

If we assume that this captured all the job listing over 2016 to 2017 (probably unlikely given the huge variance over the period and that there are months where there is little to no listing), it would seems that most jobs are posted in March in 2016 ( Start of year-ish), followed by Nov 2015 (End of year). I wonder if this coincide with the timing of graduation/bonus etc.


## Location

Finally we are at location, I have to admit that the reason I went ahead with ggvis is predominaly because of their map package.


```{r}
head(mdt$joblocation_address)

```

Some job listing are listed at multiple location, such as 
"Delhi NCR, Mumbai, Bengaluru, Kochi, Greater Noida, Gurgaon, Hyderabad, Kozhikode, Lucknow". Hence, will have to use a strsplit with mutiple arguement for ",", "/", and " ".


### Most Popular Location
```{r}
# splitting by ",", "/", and " "
splits <- max(lengths(strsplit(mdt$joblocation_address, ",|\\/|\\,| ")))
mdt <- mdt[,paste0("locationS", 1:splits) := tstrsplit(joblocation_address, ",|\\/|\\,| ")] 
  
# this will also allow job with multiple location listing to be listed to different location (duplicates)
mdt.melt.location <-  melt(mdt,  measure.vars = patterns("^locationS"))

# Setting  empty strings to NA
mdt.melt.location[value==""] <- NA

# Trimming white shape fromt he value
mdt.melt.location$value <- (trimws(mdt.melt.location$value))

#plotting most popular location for job listing
mdt.melt.location  %>%
  .[!is.na(value),.N,by=list(value)]%>%
  .[,.SD[order(-N)]] %>%
  .[, head(.SD, 10),]%>%
  .[,value:=fct_inorder(factor(value))] %>% # fct_infreq from "forcats" to reoder the factor in x axis
  ggvis(x=~value, y=~N) %>% 
  layer_bars(fillOpacity := 0.1)%>%
  add_axis("x", title = "location",properties = axis_props(
    labels = list(angle = 45, align = "left", fontSize = 10)
  )) %>%
  add_axis("y", title = "Number of posting")

```

Note that this is not extensive, sinec the job listings could have a slightly varaible in the name and it would be registered as a valid count. Ie, location such as "India-Karnataka-Bangalore"are listed as seperated instance with  "Bangalore". 

Regardless, it seems Bengaluru, Bangolore and Mumbai are among the most popular job location.


## Job description


Example output of jobsdescription

```{r}
head(mdt$jobdescription, 2)
```

It seems this is not very well formated. Some of the useful information such as payrate and indsutry is also already extracted into individual column by pre-crawler. Still, it would be interesting to see if we can indentify additional useful information for job description.

```{r}
require(tm)
require(quanteda)
require(wordcloud)
require(slam) # for row_sums
```


```{r}
clean_VC <- VCorpus(VectorSource(mdt$jobdescription)) %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(tolower)  %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(PlainTextDocument) %>%
  tm_map(stemDocument, language = "english")

clean_VC.tdm <- TermDocumentMatrix(clean_VC)

# Calculate the rowSums: term_frequency
# Notice that tdm is actually simple-triplet_matrix class,
# which is record as sparse matrix and has its own rowsum function
# class(clean_VC.tdm)
#  "TermDocumentMatrix"    "simple_triplet_matrix"

term_frequency<-row_sums(clean_VC.tdm)


# Sort term_frequency in descending order
term_frequency<- sort(term_frequency,decreasing = T)

# Plot a barchart of the 10 most common words
barplot(term_frequency[1:10], col = "tan", las = 2)

# Create word_freqs
word_freqs <- data.frame(term = names(term_frequency), num = term_frequency)
head(word_freqs)


wordcloud(word_freqs$term, word_freqs$num, max.words = 100, 
           random.order=FALSE, rot.per=0.35, colors=brewer.pal(8,"Dark2"))
```

```{r}

clean_VC_sparse_r40<-removeSparseTerms(clean_VC.tdm, 0.4)
tdm<-sort(rowSums(as.matrix(clean_VC_sparse_r40)), decreasing  = TRUE)
word_freqs <- data.frame(term = names(tdm), num = tdm)

set.seed(142)   
wordcloud(word_freqs$term, word_freqs$num, min.freq=20, scale=c(5, .1), colors=brewer.pal(6, "Dark2"))   
```

```{r}
clean_VC_sparse_r10<-removeSparseTerms(clean_VC.tdm, 0.1)
 tdm<-as.matrix(clean_VC_sparse_r10)
 
comparison.cloud(tdm, colors = c("orange", "blue"), max.words = 50)
```


```{r}
library(RColorBrewer)
commonality.cloud(tdm, random.order=FALSE, scale=c(5, .5),colors = brewer.pal(4, "Dark2"), max.words=400)
```


```{r}
docs <-VC %>%
  tm_map(removePunctuation) %>%
  tm_map(removeNumbers) %>%
  tm_map(tolower)  %>%
  tm_map(removeWords, stopwords("english")) %>%
  tm_map(stripWhitespace) %>%
  tm_map(PlainTextDocument)

myCorpusCopy<- docs ## creating a copy to be used as a dictionary

# docs <- docs %>%
#   tm_map(stemDocument) %>%
#   tm_map(stemCompletion, dictionary = myCorpusCopy)

docs.tdm<-TermDocumentMatrix(docs)

term_frequency<-row_sums(clean_VC.tdm)

# Sort term_frequency in descending order
term_frequency<- sort(term_frequency,decreasing = T)

# Plot a barchart of the 10 most common words
barplot(term_frequency[1:10], col = "tan", las = 2)

require(wordcloud)

# Create word_freqs
word_freqs <- data.frame(term = names(term_frequency), num = term_frequency)
head(word_freqs)


wordcloud(word_freqs$term, word_freqs$num, max.words = 100, 
           random.order=FALSE, rot.per=0.35, colors=brewer.pal(8,"Dark2"))



```


```{r}
inaugdfm <- dfm(mdt$jobdescription, remove = stopwords("english"), 
                stem = TRUE, groups=mdt$industry1, ngrams = 2)
inaugdfm <- dfm(mdt$jobdescription)
                
                
tdm_trim<-t(dfm_trim(inaugdfm, max_docfreq = .8))
head(tdm_trim)

term_frequency<-row_sums(tdm_trim)

# Sort term_frequency in descending order
term_frequency<- sort(term_frequency,decreasing = T)

word_freqs <- data.frame(term = names(term_frequency), num = term_frequency)

wordcloud(word_freqs$term, word_freqs$num, max.words = 100, 
           random.order=FALSE, rot.per=0.35, colors=brewer.pal(8,"Dark2"))


```

```{r}
docs<-c(mdt$jobdescription, mdt$industry1)
data(data_corpus_inaugural)


data.frame(inaugSpeech = texts(mdt$jobdescription), docvars(as.factor(mdt$industry1)))
            
            
myCorpus <- corpus_subset(data.frame(mdt$jobdescription,)


myStemMat <- dfm(myCorpus, remove = stopwords("english"), stem = TRUE, remove_punct = TRUE)
myStemMat[, 1:5]

```

