---
title: "kaggle-naukri-jobdb"
author: "minghao"
date: "October 21, 2017"
output: 
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    df_print: tibble
    code_folding: hide
---

# Intro

The dataset is given on the context that this is a pre-crawled dataset, taken as subset of a bigger dataset that was created by extracting data from Naukri.com, which looks to be a leading job board in India.

sadly it is not known how the data were subseted, whether how trends in this data sets would translate to actual trends in actual job market, though we can always speculate. 


# libraries
```{r, warning = FALSE, message=FALSE}
require(tidyverse)
require(data.table)
require(lubridate)
require(stringr)
require(ggvis)
require(ggplot2)
require(forcats)
require(ggmap)
require(highcharter)
require(broom)
require(plotly)
```

```{r}
# setwd(dir="D:\\Users\\gdrtmh\\Desktop\\Kaggle\\Jobs on Naukri")
inputdt <- as.data.table(read.csv(file="naukri_com-job_sample.csv", strip.white = TRUE, 
                             na.strings=c("","NA","Not Mentioned","Not Disclosed by Recruiter"), 
                             stringsAsFactors = FALSE))

#create a duplicate of inputdt for modification
mdt <- inputdt

# setting "Not mentioned" as NA for experience etc.
# setting "Not Disclosed by Recruiter" that is the norm for NA in payrate
```

I have set rmarkdown to  hide the code by default, so that everyone can glimpse whatever analysis/exploration thereis without running into a wall of text. One can easily shown the code if it appears interesting.


# Questions

1. How particular industry or experience influence the disclosure of salary.

2.


# Data Exploration & Basic Analysis
This section seems to clean and organise the input data, while also investigate its basic quality/distribution.

```{r}
glimpse(mdt)
```

Subsections below are exploration of particular column. The purpose of some of the columns aren't clear defined and explained, while others require prepocessing before going futher with exploration.



## Number of Missing data

```{r}
NA_rows_numbers<-sapply(mdt, function(x) sum(is.na(x)))
Valid_rows_numbers<-sapply(mdt, function(x) sum(!is.na(x)))
data.table(names(NA_rows_numbers), NA_rows_numbers, Valid_rows_numbers)
```
The amount of field with inputs and NA with respective column can be observed here. Sadly, probably the most interesting aspect (pay) is mostly empty. Though perhaps we can examine later is how common and prevalence is this practice.


## Site_name

```{r}
## summary(as.factor(inputdt$site_name))
# www.naukri.com           NA's 
#           3987          18013 
mdt$site_name <- NULL
```

 **site_name** is filled with either www.naukri or empty fill. This column is removed as it would have little effect on subsequent data analysis/exploration. This column shall be discarded.

## Uniq_id

```{r}
## uniqueN(inputdt$uniq_id)
# [1] 22000
head(mdt$uniq_id,2)
```

This Would be useful as an unique identifier for individual job listing. As it's a corresponding long string that is unique to each individual job listing.

## Jobid

```{r}
## uniqueN(inputdt, by=c("jobid"))
# [1] 21910
head(mdt$jobid,2)
```

It is not exactly clear how "jobid" is structed, yet there are 21910 ( just slightly less than uniq_id's row number) unique instances of jobid. As this number is only slightly less than that of "unique_id", I am guessing it might be capturing the relisting of certain jobs, this wil be explored in later sections.


## Payrate

Here comes the interesting part, Salary. 



```{r}
#printing the head of of non NA column for payrate
head(mdt[!is.na(mdt$payrate)]$payrate, 15)
```


Though,it doesn't appear to be very well structured judging from the sample output of this column


I choosed to processed this column by:

1. Extract the string before "P.A". This mean that incentives/ joining bonus that is typically listed after salary P.A ( Per Annum) are ignored, not that there is any solid to pull from them.

2. Removing some of the values that is listed as per hour

3. Extract only numeric values in  the "X-Y" range 



```{r}
## 1. Extract the string before "P.A".
# Seeing that payrate listing would corresponds to format of "1,50,000 - 2,50,000 P.A"
# I would just use gsub to extract strings before P.A
mdt$payrate.M<-gsub( " P.A.*$", "", mdt$payrate)
# this is essential to remove numerical characters after the P.A string
# this also removed certain listed incentive/commission information in payrate
# some values are exceedingly low (relatively to one another, but it doesn't seems like the values it is recorded in monthly compensation format either.

## 2. Removing some of the values that is listed as per hour
mdt[str_detect(payrate.M,"per hour"), payrate.M := NA]

## 3. Extract only numeric values in  the "X-Y" range 
# stripping all the non numerical chr from the payrate
mdt$payrate.M<-gsub("[^0-9-]", "", mdt$payrate.M)
# mdt$m.payrate<-gsub("[^0-9,-,-]", "", mdt$m.payrate)

```


Some of the payrate that isn't formated. The string field has more than one dasg.  the same way are listing their P.A compensation in bracket instead

Strangely it seems like some values here in this particular payrate listing is listed as "I" instead of "1". I am not sure if this is the result of input error from recruiter or the crawler that generated this datasets. Anyhow, this need to be corrected.

```{r}
# Example of some payrate that is not extracated properly ( with 2 "-" etc after regexp treatment), 
# seeing there is no easy solution and that there is only ~200 of them, them will be ignored for now
mdt[lengths(strsplit(mdt$payrate.M, "-"))>2, list(payrate,payrate.M),]


mdt[lengths(strsplit(mdt$payrate.M, "-"))>2, payrate.M:=str_extract(payrate, "(?<=\\(Rs.)(.*?)(?=\\))")]
# RegExp Explanation
#(?<=\\(Rs.) escape and extract strings after (Rs.
#(.*?) strings within with wildcard
#(?=\\))") escape and extract strings before ) payrate.M:=

# Correction for special case where "1" is recorded as "I"
mdt[substr(mdt$payrate.M,1,5)=="I5600",payrate.M:="15600-39100"]
#Pay Scale: PB-3 (Rs.I5600-39100) with AGP of Rs.8000/- p.m.


```

With all the preprocessing above, we currently have:

```{r}
# Removing some empty string introduced in the preprocessing process.
mdt[payrate.M=="", payrate.M:= NA]

# # Number of valid rows in payrate
# sum(!is.na(mdt$payrate.M))
# [1] 5270
```

Splitting the payrate columns that have been properly formated so far, create two column that is indicative of the low and high range of the compensation. Then, create a column **m.payrate.Mean** as to recorded the mean of the offered compensation based on the range.

```{r}
# Identifying the appropriate number of splits for payrate with str "-"
splits <- max(lengths(strsplit(mdt$payrate.M, "-")))

# Spliting the payrate into m.payrate1 and m.payrate2, Ignoring those column with additional splits
mdt <- mdt[lengths(strsplit(mdt$payrate.M, "-"))<=2,
               paste0("m.payrate", 1:2) := tstrsplit(payrate.M, "-", fixed=TRUE)][] 

# Changing the column into numerics
mdt$m.payrate1<- as.numeric(mdt$m.payrate1)
mdt$m.payrate2<- as.numeric(mdt$m.payrate2)

# account for case where there is only one listed payrate instead of a range
mdt<-mdt[is.na(m.payrate2), m.payrate2:=m.payrate1]

# Creating a column that indicate the mean of the payrate, would be easier on plotting etc
# na.rm = T to enable those listing that only depicted single number of payrate to have a mean
mdt[, m.payrate.Mean := rowMeans(.SD), by = uniq_id, .SDcols = c("m.payrate1", "m.payrate2")]
```

The mean value and median value of the payrate should almost be equivalent given that in this case, they are both determined from the listed upper and lower window of payrate. 

```{r}
# Checking if the values in experience1 are always lower than experience2, 
# such that experience1 can be considered as a lower limit for the job, vice versa for experience2
# summary(mdt$m.payrate1>mdt$m.payrate2)
#    Mode   FALSE    TRUE    NA's 
# logical       2    4743   17255 

# listing the data with m.payrate1 > m.payrate2, removing these data
mdt[m.payrate1>m.payrate2,
    list(payrate, payrate.M)]


mdt[(m.payrate1>m.payrate2),`:=`(payrate.M= NA,
                                 m.payrate1= NA,
                                 m.payrate2= NA,
                                 m.payrate.Mean= NA)]
```

In both of these cases, the payrate is not well formated and extracted. There is probably some bad extraction in the extracting of payrate that is not depicted here as well as they may be hidden behind m.payrate2 > m.payrate1.

Removing these cases (where m.payrate1>m.payrate2) by setting them to NA.


```{r}
# After attempting a few threshold of cut, I find 9e+6 to be most suitable, listed below are a table depicted payrate after this threshold.
mdt[m.payrate.Mean>9e+6] %>%
  .[,list(payrate, m.payrate1, m.payrate2)]

# for an easy way out, I will be removing all extracted payrate data for these badly formated
mdt[m.payrate.Mean>9e+6,`:=`(payrate.M= NA,
                             m.payrate1= NA,
                             m.payrate2= NA,
                             m.payrate.Mean= NA)]
```

In fact, it almost seems like the posting of payrate = "Pay Band:  PB4 (Rs.3740067000) with Academic Grade Pay of Rs.8,000/p. m." are just reposting. As there exist a host of rows with such adata.


```{r}
#total count of valid payrate records
N_valid_payrate<-sum(!is.na(mdt$m.payrate.Mean))

payrate_record_percentage <- sum(!is.na(mdt$m.payrate.Mean))/sum((mdt$m.payrate.Mean), na.rm=T)
```

Sadly, there is only `payrate_record_percentage` of the job listing that has its payrate listed and extracted so fr, that would be a mere `N_valid_payrate` records.


### Histogram of PayRate

```{r}
mdt[!is.na(m.payrate.Mean),][m.payrate.Mean<9e+6] %>%
  melt(.,  measure.vars = patterns("^m.payrate")) %>%
  ggvis(~value, fill = ~variable) %>%
  group_by(variable) %>%
  layer_densities() %>%
  add_axis("x", title = "Payrate Per Annum") %>%
  add_axis("y", title = "Density")



```

Would be interesting to see the how the payrate woudl vary per industry/skill/location in a later section. Though, if we narrow down our windowing.


```{r}
mdt[!is.na(m.payrate.Mean),][m.payrate.Mean<2e+6] %>%
  melt(.,  measure.vars = patterns("^m.payrate")) %>%
  ggvis(~value, fill = ~variable) %>%
  group_by(variable) %>%
  layer_densities() %>%
  add_axis("x", title = "Payrate Per Annum") %>%
  add_axis("y", title = "Density")

```

Well it seems that for the payrate of the listed jobs, the median of listed salary is just about ~300k repee PA ( since the jobboard seems to be based in india).

## Education

```{r}
head(mdt$education,5)
```

It can be seen ( after skimming through multiple lines)that Education can be split into different field quite accurately with delimiter of  "UG:", "PG:" and "Doctorate:". Also, in some case, the education field can be listed, yet are noted as "Not Required" (such as Doctorate:Doctorate Not Required)


```{r}
mdt %>%
  .[grep("Diploma",education),m.Education.tf.Diploma := TRUE] %>% #Diploma
  .[,m.Education.UG :=trimws(str_extract(education, "(?<=UG: ).*(?=PG:)|(?<=UG: ).*$"))] %>%
  .[grep("Not Required",m.Education.UG),m.Education.UG :=NA] %>% #Removing "not required"
  .[!is.na(m.Education.UG),m.Education.tf.UG:=TRUE] %>%
  .[,m.Education.PG :=trimws(str_extract(education, "(?<=PG:).*(?=Doctorate:)|(?<=PG:).*$"))] %>%
  .[grep("MBA",m.Education.PG),m.Education.tf.MBA := TRUE] %>% #MBA
  .[grep("Not Required",m.Education.PG),m.Education.PG :=NA] %>%
  .[!is.na(m.Education.PG),m.Education.tf.PG:=TRUE] %>%
  .[,m.Education.DR :=trimws(str_extract(education, "(?<=Doctorate:).*$"))] %>%
  .[grep("Not Required",m.Education.DR),m.Education.DR :=NA] %>%
  .[!is.na(m.Education.DR),m.Education.tf.DR:=TRUE]

```

Note that I also created some "m.Education.tf.x" (where x= UG, Diploma, PG, MBA, DR ) to indicate the need for designated education level in True or NA.

In some case, such as

```{r}
mdt[(33:35)]$m.Education.UG
```

Additional split on "," will allow more details to be extracted(Ie, B.Sc, B.Pharma requirement etc), though I would not venture further on this direction for now.

### Number of job listing for a particular level of Education

```{r}
melt(mdt[,lapply(.SD, sum, na.rm=TRUE) ,.SDcols= (names(mdt) %like% "^m.Education.tf*")], measure.vars = patterns("^m.Education.tf*"))
```

 The Majority of Job posting listed UG(Undergraduate) as Educational requirement. Also, Curiously, there PG(Postgraduate) is listed for many listings too.

### Education Hierarchy Factor

Note that the number of job listing table on top allowed mulitple entries of job for each different ncategories of education. For example, A job that listed education requirement as UG: and PG: would be counted as 1 in both categories. 

The following codes created a section that would only attach the highest recorded education for everyt individual job listing. Ie, if the job listed for "UG: B.Sc, PG: Masters", it would be recorded as Master level job in this column. I assume the level of education follow a nested/linear path that corresponds to: Diploma > UG > MBA > PG > DR. The position of MBA is perhaps slightly controversial.

```{r}
# This list of data table chain simply utilise grep(str_detect) and is.na( the existance of UG, PG, DR) in previous education columns to assign a unique factor to the column

mdt %>%
  .[grep("Diploma",education),m.Education.factor := "Diploma"] %>% 
  .[!is.na(m.Education.UG),m.Education.factor := "UG"] %>%
  .[!is.na(m.Education.PG),m.Education.factor := "PG"] %>%
  .[grep("MBA",education),m.Education.factor := "MBA"] %>%
  .[!is.na(m.Education.DR),m.Education.factor := "DR"]
  
mdt$m.Education.factor <- factor(mdt$m.Education.factor)


# There outough to be better methods to melt, extract the highest education requirement information but I failed to extracted/melt it cleanly from the previous extracted information.
```

## Skills

```{r}
#head of records in skill
head(mdt$skills)

# unique(inputdt$skills)
```

listing of top 10 skill in demand

```{r}

  ## gglot 
fill <- "#4271AE"
lines <- "#1F3552"


mdt[!is.na(skills),.N,by=list(skills)] %>%
  .[,.SD[order(-N)]] %>%
  .[, head(.SD, 20),] %>%
  .[,value:=fct_inorder(factor(skills))] %>% # fct_infreq from "forcats" to reoder the factor in x axis
  ggplot() +
  geom_bar(aes(x = reorder(skills, -N),  y = N), 
           stat="identity",  
           colour = lines, fill = fill) +
        scale_y_continuous(name = "Number of listing") +
        scale_x_discrete(name = "Skill") +
        ggtitle("Number of listing for particular skill") +
        theme_bw() +
        theme(panel.grid.major = element_line(colour = "#d3d3d3"),
              panel.grid.minor = element_blank(),
              panel.border = element_blank(),
              panel.background = element_blank(),
              plot.title = element_text(size = 14, face = "bold"),
              axis.title = element_text(face="bold"),
              axis.text.x = element_text(colour="black", size = 8, angle = 90,
                                         hjust = 1,  vjust = 0.5),
              axis.text.y = element_text(colour="black", size = 9),
              axis.line = element_line(size=0.5, colour = "black"))
  
  # ggvis(x=~skills, y=~N) %>%
  # layer_bars(fillOpacity := 0.1) %>%
  # add_axis("x", properties = axis_props(
  #   labels = list(angle = 45, align = "left", fontSize = 10)
  # ))


 mdt$skills <- as.factor(mdt$skills)
```

This shows that the most demanding skills are IT Software - Application Programming, followed by Sales and ITES.



## Industry
```{r}

# Checking whether every val  id entry has "yrs" in the records, 
# summary(!is.na(mdt$industry))
#    Mode   FALSE    TRUE    NA's 
# logical       5   21995       0 
# Majority of the job listing did listed valid industry

head(mdt$industry)

# since there is limited number of combination of industry, this column should be set as a factor
mdt$industry <- as.factor(mdt$industry)
```
It seems most job listing are well arranged, it is almost listed in tree like structure that.

```{r}
mdt[, .N, by= industry] %>%
  .[,.SD[order(-N)]] %>%
  .[,head(.SD, 20)] %>%
    ggplot() +
  geom_bar(aes(x = reorder(industry, -N),  y = N), 
           stat="identity",  
           colour = lines, fill = fill) +
        scale_y_continuous(name = "Number of listing") +
        scale_x_discrete(name = "Skill") +
        ggtitle("Number of listing for particular skill") +
        theme_bw() +
        theme(panel.grid.major = element_line(colour = "#d3d3d3"),
              panel.grid.minor = element_blank(),
              panel.border = element_blank(),
              panel.background = element_blank(),
              plot.title = element_text(size = 14, face = "bold"),
              axis.title = element_text(face="bold"),
              axis.text.x = element_text(colour="black", size = 8, angle = 90,
                                         hjust = 1,  vjust = 0.5),
              axis.text.y = element_text(colour="black", size = 9),
              axis.line = element_line(size=0.5, colour = "black"))



```


### Least popular Industry

```{r}
mdt[, .N, by= industry] %>%
  .[,.SD[order(-N)]] %>%
  .[!is.na(industry), tail(.SD, 10),] %>%
  .[,industry:=fct_inorder(factor(industry))] %>% # fct_infreq from "forcats" to reoder the factor in x axis
  ggvis(x=~industry, y=~N) %>% 
  layer_bars(fillOpacity := 0.1) %>%
  add_axis("x", title = "Industry",properties = axis_props(
    labels = list(angle = 45, align = "left", fontSize = 10)
  )) %>%
  add_axis("y", title = "Number of posting")


```



## Experience

sample data
```{r}
head(mdt$experience)
```

All processing it require it just the extraction of the numerics values.

```{r}
# dt[!str_detect(dt$experience, "yrs")]# It seems all the listing are in a matter of years

# one entry with  double dash "-"
# glimpse(dt[max(lengths(strsplit(dt$experience, "-")))==lengths(strsplit(dt$experience, "-"))])
mdt[max(lengths(strsplit(mdt$experience, "-")))==lengths(strsplit(mdt$experience, "-"))]$experience <- "1 - 3 yrs"

# Removing the last 3 char - "yrs" in the experience records
# nchar(mdt$experience, allowNA = TRUE)-3
mdt$experience.M <- str_sub(mdt$experience, 1,nchar(mdt$experience, allowNA = TRUE)-4)

# Splitting the experience
splits <- max(lengths(strsplit(mdt$experience.M, "-")))
mdt <- mdt[,paste0("experience", 1:splits) := tstrsplit(experience.M, "-", fixed=TRUE)][]

# Changing them to appropriate class
mdt$experience1<-as.numeric(mdt$experience1)
mdt$experience2<-as.numeric(mdt$experience2)

# Create a columm for median of experience
mdt[, m.experience.Median:=floor(median(c(experience1, experience2))), by = uniq_id]
#floor to make sure the median experience remain a integer ( round down) 


# Checking if the values in experience1 are always lower than experience2, 
# such that experience1 can be considered as a lower limit for the job, vice versa for experience2
# summary(mdt$experience1<=mdt$experience2)
#    Mode    TRUE    NA's 
# logical   21885     115 

# renaming the experience1 and experience2 column into more obvious form
setnames(mdt, c("experience1","experience2"), c("m.experience.L", "m.experience.U"))


mdt[!is.na(m.experience.L),]%>%
  melt(.,  measure.vars = patterns("^m.experience")) %>%
  ggvis(~value, fill = ~variable) %>%
  group_by(variable) %>%
  layer_densities(adjust = 2) %>%  #adjust parameter to smooth the wiggliness of density
  add_axis("x", title = "Number of years") %>%
  add_axis("y", title = "Density")
```



To explore experience variable with other factors, I will aggregate the listed Experience into different groups, in a matter of groups of 3 years. This simply aggregate/dice Experience to larger groups

```{r}
mdt<-mdt[, m.experience.Cut := cut(m.experience.Median, 
                                   breaks=c(-Inf, 3*(0:3), 10+4*(0:2), Inf), right=F)]
mdt[is.na(m.experience.Cut)]$m.experience.Cut <- "Not listed"
# By default NA  ( those not in the range of the cut are outside our boundaries and not counted. Started the break with -Inf to include them in the calculation
```


## Number of positions

```{r}
# summary(mdt$numberofpositions)
# Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's 
#    1.00    2.00    4.00   45.12   10.00 2000.00   17536 
```
Intestingly, there are job posting listed to have 2000 positions available, a glimpse at this particular listing is shown below.

```{r}
mdt[numberofpositions==2000]
```

The majority of the job listing doesn't list a number of the number of positions, I think it is reasonable to assump those without listed number in 'Number of position' as 1.

```{r}
mdt<-mdt %>%
  .[, m.Npositions:=numberofpositions] %>%
  .[is.na(numberofpositions), m.Npositions:=1]

summary(mdt$m.Npositions)
```

Now all recorded number of positions have a valid number.

## Date Time

split and time zone
```{r}
head(mdt$postdate)

#str split the postdate based on format corresponds to "2016-05-21 19:30:00 +0000" on "+"
mdt[, c("postdate_time","postdate_timezone") := tstrsplit(postdate,"+",2)]

# summary(as.factor(mdt$postdate_timezone))
# 0000  NA's 
# 21977    23 
# it seems timezone is pretty meaningless

# Removing timezone column, as timezone data is unnecessary. 
mdt$postdate_timezone <- NULL

```

Formatting into appropriate time Series input

```{r}
mdt$postdate_time <- parse_date_time(mdt$postdate_time, "%Y-%m-%d H:M:S")
#ignoring timezone for now, shouldn't make any difference too

min_time<-min(mdt$postdate_time, na.rm =T)
max_time<-max(mdt$postdate_time, na.rm =T)
```

The datasets job listing ranged from `min_time` to `max_time`.

### Number of job posting over time

```{r}
mdt[, m.timeCut := cut(as.POSIXct(postdate_time), breaks="1 month")]

mdt[!is.na(m.timeCut), .N, by=m.timeCut] %>%
  ggvis(~m.timeCut, ~N) %>%
  layer_bars(fillOpacity := 0.1) %>%
  add_axis("x", properties = axis_props(
    labels = list(angle = 45, align = "left", fontSize = 10)
  ))

# alternative data table method
# mdt[, .N, by=month(postdate_time)] 

# alternate dplyr solution
# mdt$uniq_id
# mdt %>% 
#   group_by(yr = year(postdate_time), mon = month(postdate_time)) %>% 
#   summarise(mn_amt = length(uniq_id))

```

If we assume that this captured all the job listing over 2016 to 2017 (probably unlikely given the huge variance over the period and that there are months where there is little to no listing), it would seems that most jobs are posted in March in 2016 ( Start of year-ish), followed by Nov 2015 (End of year). I wonder if this coincide with the timing of graduation/bonus etc.


## Location

Finally we are at location, I have to admit that the reason I went ahead with ggvis is predominaly because of their map package.


```{r}
head(mdt$joblocation_address,5)
```

Some job listing are listed at multiple location, such as 
"Delhi NCR, Mumbai, Bengaluru, Kochi, Greater Noida, Gurgaon, Hyderabad, Kozhikode, Lucknow". Hence,  a strsplit with arguement for "," and "/", should do the job.


### Most Popular Location

```{r}
# Removing str in parenthesis as this information is redundant for our purpose of data exploration
mdt$joblocation_address <- gsub("\\(.*\\)","", mdt$joblocation_address)

# splitting by ",", "/", and " "
splits <- max(lengths(strsplit(mdt$joblocation_address, ", |/")))
mdt <- mdt[,paste0("m.locationS", 1:splits) := tstrsplit(joblocation_address, ", |/")] 
mdt$joblocation_address<- NULL # removing the original data
# some joblocations appears to use space, " " as delimiter... 
# Obviously, implentating strsplit on " " would introduce additional issues (ie location name with space. Thankfully this phenomena is rathe rare, I would just ignore " " as a delimiter for now.
  
# this will also allow job with multiple location listing to be listed to different location (duplicates), for example 
mdt.melt.location <-  melt(mdt,  measure.vars = patterns("^m.locationS"), value.name = "m.i.location")
# where m.i.location indicates modified and individual extracted location

#removing variable that just indicate the order of the "m.locationSX" in Location
mdt.melt.location$variable <-  NULL


# Setting  empty strings to NA
mdt.melt.location[m.i.location==""] <- NA

# Trimming white space from the value
mdt.melt.location$m.i.location <- trimws(mdt.melt.location$m.i.location)

# plotting most popular location for job listing
mdt.melt.location  %>%
  .[!is.na(m.i.location),.N,by=list(m.i.location)] %>%
  .[,.SD[order(-N)]] %>%
  .[, head(.SD, 10),] %>%
  .[,value:=fct_inorder(factor(m.i.location))] %>% # fct_infreq from "forcats" 
      ggplot() +
  geom_bar(aes(x = reorder(m.i.location, -N),  y = N), 
           stat="identity",  
           colour = lines, fill = fill) +
        scale_y_continuous(name = "Number of listing") +
        scale_x_discrete(name = "location") +
        ggtitle("Number of listing for particular location") +
        theme_bw() +
        theme(panel.grid.major = element_line(colour = "#d3d3d3"),
              panel.grid.minor = element_blank(),
              panel.border = element_blank(),
              panel.background = element_blank(),
              plot.title = element_text(size = 14, face = "bold"),
              axis.title = element_text(face="bold"),
              axis.text.x = element_text(colour="black", size = 8, angle = 90,
                                         hjust = 1,  vjust = 0.5),
              axis.text.y = element_text(colour="black", size = 9),
              axis.line = element_line(size=0.5, colour = "black"))
  
  # ggvis(x=~m.i.location, y=~N) %>% 
  # layer_bars(fillOpacity := 0.1) %>%
  # add_axis("x", title = "location",properties = axis_props(
  #   labels = list(angle = 45, align = "left", fontSize = 10)
  # )) %>%
  # add_axis("y", title = "Number of posting")

```

Note that this is not extensive, since the job listings could have a slight variation in the name and it would hence not be registered in a same groups for counting. Ie, location listed as "Bengaluru" are not counted in the same instance as with "Bangalore"(Eventhough they appears to be on almost identical location on map). The current setup should be sufficient in showing the popularity of certain locations here.

Regardless, it seems Bengaluru, Bangolore and Mumbai are among the most popular job location.


## Job description

Example output of jobs description column

```{r}
head(mdt$jobdescription, 1)
```

It seems this is not very well formated. Some of the useful information such as payrate and indsutry is also already extracted into individual column by pre-crawler. Still, it would be interesting to see if we can indentify additional useful information for job description.

It seems every job description begins with  "Job Description Â  Send me Jobs like this". This word structure should be removed before any further comparison



```{r}
## Removing the "Job Description Â  Send me Jobs like this" 
mdt$jobdescription <- str_replace(mdt$jobdescription, "Job Description", "") %>%
  str_replace(., "Send me Jobs like this", "")

# Checking the output
head(mdt$jobdescription, 1)
```

```{r, warning = FALSE, message=FALSE}
require(tm)
require(quanteda)
require(wordcloud)
require(slam) # for row_sums
require(RColorBrewer)
```

```{r}
### create Document Feature Matrix 
dfm.txt <- corpus(mdt$jobdescription, docvars = data.frame(grp = mdt$industry)) %>%
  dfm(., groups = "grp", 
      # stem = T, 
      remove=stopwords("english"),
      remove_symbols = T,
      remove_punct = T,
      remove_numbers = T,
      remove_url = T)

dfm.txt
```

### Barplot of popular terms in job 

```{r}
## Data.Frame of Term frequency
dt.freq<-data.frame(Language = names(topfeatures(dfm.txt,20)),
                            N=topfeatures(dfm.txt,20))

## Barplot of Term frequency
setDT(dt.freq)[,Language:= fct_inorder(factor(Language))] %>%
  ggvis(~Language, ~N) %>% 
  layer_bars(fillOpacity := 0.1) %>%
  add_axis("x", title = "Programming Language",
           properties = axis_props(labels = list(angle = 45, align = "left", fontSize = 10))) %>%
  add_axis("y", title = "Number of instance Mentioned")

```

### Wordcloud of popular terms

```{r, warning = FALSE}
##warning =False in the knit/rmd to silence the warning of words that could not be fit on the page

## Rowsum to find individual term's frequency across all documents
term_frequency<-row_sums(t(dfm.txt))

## Sort term_frequency in descending order
term_frequency<- sort(term_frequency,decreasing = T)

## Create word_freqs
word_freqs <- data.frame(term = names(term_frequency), num = term_frequency)

set.seed(4211)
## Plot wordcloud
wordcloud(word_freqs$term, word_freqs$num, max.words = 100,
           random.order=FALSE, colors=brewer.pal(8,"Dark2"), rot.per=.3, scale=c(3,.5))
```

Although, given that IT industry constitute the majority of the listing, it is to narrow down to per industry basics to look at the differences and significant.


### Comparison of wordcloud by industry

extracting the list of top 8 industry in terms of their number in the job listing.

```{r}
## Summarizing and identifying the most popular 8th industry 
popular_industry_count <- mdt %>%
  .[!is.na(industry),.N,by=list(industry)] %>%
  .[,.SD[order(-N)]] %>%
  .[, head(.SD, 8),]

# popular_industry_count$industryS1
```


```{r, warning = FALSE}
# tfidf(wikiDfm)

textplot_wordcloud(dfm_select(dfm.txt, documents=popular_industry_count$industry[1:4]), 
                   comparison = T, rot.per = .25, scale=c(2,.55),title.size =1)

textplot_wordcloud(dfm_select(dfm.txt, documents=popular_industry_count$industry[5:8]), 
                   comparison = T, rot.per = .25, scale=c(2,.55),title.size =1)
```



### Extracting Specific frequency of strings - Programming Langauges

We can also explore the term specific mention of specific strings that corresponds to programming languages to glimpse their popularity and value in job posting 

```{r}
## Making a dictionary list of common and popular string list of programming languages
myDict <- dictionary(list(git="git",
                          c = "c",
                          python = "python",
                          java="java",
                          cpp = "c++",
                          chash= "csharp",
                          r="r",
                          javascript="javascript",
                          php= "php",
                          go="go",
                          swift="swift"
                          ))
# the later two langauge might have issue identifying the correct frequency of the word as the names can also double as common word.

## Applying the dictionary just defined on the job description column.
ProgrammingL_popularity <- dfm(mdt$jobdescription, dictionary = myDict)

## Topfeatures() to extract popular languages and its counts
dt.ProgrammingL<-data.frame(Language = names(topfeatures(ProgrammingL_popularity)),
                            N=topfeatures(ProgrammingL_popularity))

## fct_inorder to reorder the langauge with the 
setDT(dt.ProgrammingL)[,Language:= fct_inorder(factor(Language))] %>%
  ggvis(~Language, ~N) %>% 
  layer_bars(fillOpacity := 0.1) %>%
  add_axis("x", title = "Programming Language",
           properties = axis_props(labels = list(angle = 45, align = "left", fontSize = 15))) %>%
  add_axis("y", title = "Number of instance Mentioned")

## To extract frequency of words
# freq <- textstat_frequency(ProgrammingL_popularity)
# "textstat_frequency" is only available into quentda 0.99, which doesn't seem to be available by default at the moment
```

Strangely, Java is the most popular languages, it was mentioned 5000 times in job description. It is follower by c and javascript, though they are only half as popular. R is also rather popular, even somehow being more popular than python too.

# Cross Data Analysis 


```{r}
# Preserving only extracted data/useful data in the dt
mdt <- mdt %>%
  .[,(colnames(.) %like% "^m."| colnames(.)=="industry"| colnames(.)=="uniq_id"), 
             with=FALSE]

# Another modified data table with with multiple listing of locations as multiple entries
mdt.melt.location <- mdt.melt.location %>%
  .[,(colnames(.) %like% "^m."| colnames(.)=="industry"| colnames(.)=="uniq_id"), with=FALSE]
```

### Industry, Payrate, Experience

Thus, if we were to restrict our boxplot to industry with more than 30 listing.

```{r}
mdt[!is.na(m.payrate.Mean), if (.N > 30) .SD, by = industry] %>%
  ggplot(aes(x = reorder(industry, -m.payrate.Mean, median), 
             y = m.payrate.Mean)) +
  geom_boxplot(colour = lines, fill = fill,
               size = 1) +
  scale_y_continuous(name = "Compensation in USD") +
  scale_x_discrete(name = "Country") +
  ggtitle("Boxplot of CompensationUSD by Country") +
  theme_bw() +
  theme(panel.grid.major = element_line(colour = "#d3d3d3"),
        panel.grid.minor = element_blank(),
        panel.border = element_blank(),
        panel.background = element_blank(),
        plot.title = element_text(size = 14, face = "bold"),
        axis.title = element_text(face="bold"),
        axis.text.x = element_text(colour="black", size = 8, angle = 90,
                                   hjust = 1,  vjust = 0.5),
        axis.text.y = element_text(colour="black", size = 9),
        axis.line = element_line(size=0.5, colour = "black"))  

# hcboxplot(x = t.filtered.mdt$m.payrate.Mean, 
#         var = t.filtered.mdt$industry) %>% 
# hc_chart(type = "column") %>% 
# hc_title(text = "Plot of Payrate(Mean, PA) vs Industry") 
```

The clear winners are "IT - Sofreware", "Internet" and "Strategy/Management consulting Firm".

In the form of table:

```{r}
mdt[!is.na(m.payrate.Mean), if (.N > 30) .SD, by = industry]%>%
  .[!is.na(m.payrate.Mean), .(.N, 
                              Q1=quantile(m.payrate.Mean, 0.25),
                              PayMed=median(m.payrate.Mean),
                              Q3=quantile(m.payrate.Mean, 0.75)), by=industry] %>%
  .[,.SD[order(-PayMed)]] %>%
  .[order(-N)]
```


On the end of tail,  some of the industries's pay are pathetic - both the median of the industry and the range ( Q3-Q1) is fairly minute. This probably shows both the starting pay and the latitude/margin to growth in these industry are limited. This is exemplify by industry in "BPO/Call Centre", "Recruitment/Staffing", "Teaching/Training" and "Healthcare". 

All this despite the fact that some of these low paying industries - "Call Centre", "Education", "Hospital" are industries that posted most job listing for the duration of the crawled data. So much for supply and demand.


```{r}
hcboxplot(x = mdt$m.payrate.Mean, 
          var = mdt$m.experience.Median) %>% 
  hc_chart(type = "column") %>% 
  hc_title(text = "Plot of Payrate(Mean, PA) vs experience (yrs)") 
```

```{r}
filtered.IMP <- mdt[!is.na(m.experience.Median) & !is.na(m.payrate.Mean),
    .SD, .SDcols=c("m.experience.Median","m.payrate.Mean")]

glance(lm(m.experience.Median~m.payrate.Mean, filtered.IMP))$adj.r.squared

tempt_ggplot <- filtered.IMP %>%
  ggplot(aes(x=factor(m.experience.Median), y=m.payrate.Mean)) +  
  geom_boxplot() +
  geom_smooth(method="lm", se=FALSE, aes(group=1)) +
    geom_text(x = 30, y = 4, label = sprintf("adj_r2: %s",
        signif(
          glance(lm(m.experience.Median~m.payrate.Mean, filtered.IMP))$adj.r.squared,
               3)), parse = TRUE)

ggplotly(tempt_ggplot) %>%
  layout(title = 'Payrate vs Experience',
         xaxis = list(title = 'Experience (years)[range]', tickangle = 90),
         yaxis = list(title = 'Annual Payrate'))

```

#### Exponentially Wage Growth? 

If we assume that Pay/Wage grew exponentially
$$Pay = Base * (1 + R) ^ T$$
$$Log(Pay) = T * Log(Base * (1 + R))$$
With regards to years

$$Log(m.experience.Median) = Log(Pay)$$

```{r}
tempt_ggplot <- filtered.IMP %>%
  ggplot(aes(x=factor(m.experience.Median), y=log(m.payrate.Mean))) +  
  geom_boxplot() +
  geom_smooth(method="lm", formula=y~x, se=FALSE, aes(group=1)) +
  geom_text(x = 30, y = 4, label = sprintf("adj_r2: %s",
        signif(
          glance(lm(m.experience.Median~log(m.payrate.Mean), filtered.IMP))$adj.r.squared,
               3)), parse = TRUE)


lm(m.experience.Median~log(m.payrate.Mean), filtered.IMP)

ggplotly(tempt_ggplot)  %>%
  layout(title = 'log(Payrate) vs Experience',
         xaxis = list(title = 'Experience (years)[range]', tickangle = 90),
         yaxis = list(title = 'log(Annual Payrate)'))

```

While both plots (with log(Pay) and pay)  have ~ 0 p-value, the adjusted r-square of linear plot are higher than that of log plot, although 0.52 it is still not very significant. But perhaps we can narrow down to specific industry in later sections and explore in full.

```{r}


hcboxplot(x = mdt$m.payrate.Mean, 
          var = mdt$m.experience.Cut,
          var2 = mdt$industry,
          outliers = FALSE) %>% 
  hc_chart(type = "column") %>%
  hc_title(text = "Plot of Payrate(Mean, PA) vs Experience (Yrs) segregated on Industry")
```

### Education vs Payrate

This section examine the payrate commanded by people with different amount of Education with respect to their level of education. 
 
```{r}
mdt.melt.Education <- melt(mdt[!is.na(m.payrate.Mean)],
     measure.vars = patterns("^m.Education.tf"), 
     na.rm=T)

hcboxplot(x = mdt.melt.Education$m.payrate.Mean, 
          var = mdt.melt.Education$m.experience.Cut, 
          var2 = mdt.melt.Education$variable,
          outliers = FALSE) %>% 
  hc_chart(type = "column") %>%
  hc_title(text = "Plot of Payrate(Mean, PA) vs Experience (Yrs) segregated on Education")
```

It seems that the level of education for fresh gradautes and people with up to 3 years worth of working experience does not resulted in a significant difference in payrate (ignoring outliers). owever, the difference in payscale of them may not appear significant when compare to other groups due to differences in magnitudes (y-axis).

On the next plot, the salary differences for poeple with little to no experience would be examined.

```{r}
# Filtering to plot for only those jobs that listed 0 to 3 job experience.
hcboxplot(x = mdt.melt.Education[m.experience.Cut=="[0,3)"]$m.payrate.Mean, 
          var = mdt.melt.Education[m.experience.Cut=="[0,3)"]$variable,
          name = "Length", color = "#2980b9") 
```

It is possible that some of the job listings that depicted no experience requirement are in fact posts that required significant amount of working experience. The extracted experience requirement may not be able to capture the exact requirement if it is not listed explicitly.

#### Alternative accounting of Education

The previous sections considers a job that listed multiple education requirement as multiple of job listing, Ie a job listed to be application to degree holder (UG), diploma, and PG would be melted into multiple entries for each level of education.

This, in effect, would lower the actual differences of payrates between different level of education. In actual market (which means the current employer market), those with advance degree are generally heavily favoured (not to say experience doesnt matter) while those with lower level of education would find it hard to compete.

Few employees, given a choice, would be willing to underemploy/undersell themselves for position that requires lower level of education or gives a loewr payrate.

```{r}
hcboxplot(x = mdt$m.payrate.Mean, 
          var = mdt$m.experience.Cut, 
          var2 = mdt$m.Education.factor,
          outliers = FALSE) %>% 
  hc_chart(type = "column") 

# table<-mdt%>%
#   .[,med.pay:=median(m.payrate.Mean), by =c("m.Education.factor", "m.experience.Cut") ]
```

Overall, it is quite obvious that assigning solely the single highest education requirement to individual job listing yield a much greater difference in their pay. I think this is a more accurate picture in reflecting the job market. 

### With regards to Industry 

```{r}

# industry_cout_table_40<-mdt[!is.na(m.payrate.Mean), count:.N, by= industry] %>%
#   .[,.SD[order(-N)]] %>%
#   head(., 40)
#   
# industry_cout_table_40$industry <- droplevels(industry_cout_table_40$industry)
# factor(industry_cout_table_40$industry)
# 
# mdt[industry_cout_table_40$industry,]
# 
# (colnames(mdt) %in% industry_cout_table_40$industry)

unique(mdt$industry)

# limited the plotting to industry with more than 100 listing
t.filtered.mdt <- mdt[!is.na(m.payrate.Mean), if (.N > 150) .SD, by = industry] %>%
  droplevels(.)

hcboxplot(x = t.filtered.mdt$m.payrate.Mean, 
          var = t.filtered.mdt$m.experience.Cut, 
          var2 = t.filtered.mdt$industry,
          outliers = FALSE) %>% 
  hc_chart(type = "column") %>%
  hc_title(text = "Plot of Payrate(Mean, PA) vs Experience (Yrs) segregated on Education")
```



```{r}
# limited the plotting to industry with more than 100 listing
t.filtered.mdt <- mdt[!is.na(m.payrate.Mean), if (.N > 110 & .N<150) .SD, by = industry] %>%
  droplevels(.)

hcboxplot(x = t.filtered.mdt$m.payrate.Mean, 
          var = t.filtered.mdt$m.experience.Cut, 
          var2 = t.filtered.mdt$industry,
          outliers = FALSE) %>% 
  hc_chart(type = "column") %>%
  hc_title(text = "Plot of Payrate(Mean, PA) vs Experience (Yrs) segregated on Education")
```

```{r}


mdt[!is.na(m.payrate.Mean)&!is.na(m.experience.Median)&!is.na(industry),
    list(intercept=coef(lm(m.payrate.Mean~m.experience.Median))[1],
         coef=coef(lm(m.payrate.Mean~m.experience.Median))[2],
         p.value=summary(lm(m.payrate.Mean~m.experience.Median))$coefficients[,4] ),
    by=industry]

mdt[!is.na(m.payrate.Mean), .N, by=industry] %>%
  .[N>10,] %>%
  .[,.SD[order(-N)]] 


mdt[!is.na(m.payrate.Mean)&!is.na(m.experience.Median)&!is.na(industry),] %>%
  group_by(industry) %>%
  do(tidy(lm(m.payrate.Mean ~ m.experience.Median, data = .)))
```

### Case study of a industry

Since we have most abundant data for the listing of jobs for "IT - Software / Software services" industry. Let's explore this particular industry in more depth.

#### IT - Software / Software services

```{r}
## Time of posting
# fitering for only data with valid payrate data and IT software industry
mdt.melt.location %>%
  .[!is.na(m.payrate.Mean)&industry=="IT-Software / Software Services",] %>%
  .[!is.na(m.timeCut), .N, by=m.timeCut] %>%
  ggvis(~m.timeCut, ~N) %>%
  layer_bars(fillOpacity := 0.1) %>%
  add_axis("x", properties = axis_props(
    labels = list(angle = 45, align = "left", fontSize = 10)
  ))
```




#### Healtcare

```{r}
## Time of posting
# fitering for only data with valid payrate data and IT software industry
mdt %>%
  .[!is.na(m.payrate.Mean)&industry=="Medical / Healthcare / Hospitals"] %>%
  .[!is.na(m.timeCut), .N, by=m.timeCut] %>%
  ggvis(~m.timeCut, ~N) %>%
  layer_bars(fillOpacity := 0.1) %>%
  add_axis("x", properties = axis_props(
    labels = list(angle = 45, align = "left", fontSize = 10)
  ))
```

This may yield false information as the trends may be results of data subseting, though, it certainly shows that most of the job listing for healthcare occured around May, March for 2016.





Plotting location require external data from geocoding ( pin pointing the locations or joining by location name) and also geojson india map mapping. Hence I am defering them to a later section



# Location difference


```{r}
## In this section, I used geocodes to query google for the lat and lon of respective locations.
# this will be done only with top 100 locations by their respective popularity 
# which incidently also only inlcudes location with more than ~10 counts of job listings.
mdt.melt.location.count.t100 <- mdt.melt.location  %>%
  .[!is.na(m.i.location),.N,by=list(m.i.location)] %>%
  .[,.SD[order(-N)]] %>%
  .[, head(.SD, 100),]
```

The results of geocoding for the top 100 cities are inserted into the kernel via output of dput()

```{r echo=FALSE, results='hide',message=FALSE}
# dput(city2latlon[,lapply(.SD, as.character)])
city2latlon<- structure(list(m.i.location = c("Bengaluru", "Bangalore", "Mumbai", 
"Hyderabad", "Delhi", "Chennai", "Secunderabad", "Noida", "Gurgaon", 
"NCR", "Pune", "Kolkata", "Greater Noida", "Ahmedabad", "Navi Mumbai", 
"Chandigarh", "Ghaziabad", "Delhi NCR", "Faridabad", "Mumbai Suburbs", 
"Kochi", "Coimbatore", "Jaipur", "Visakhapatnam", "Cochin", "Ernakulam", 
"Surat", "Lucknow", "Bengaluru Bangalore", "Vijayawada", "Bhubaneshwar", 
"Vizag", "Trivandrum", "Vadodara", "Indore", "karnataka", "Mangalore", 
"mumbai", "Nagpur", "Thane", "maharashtra", "Mysore", "Bhopal", 
"Guwahati", "Baroda", "Kanpur", "Patna", "Trichy", "Dehradun", 
"Raipur", "bangalore", "Ludhiana", "Agra", "Nasik", "Ranchi", 
"Hubli", "Cuttack", "Guntur", "delhi", "Belgaum", "haryana", 
"Ambala", "Vellore", "Madurai", "Salem", "Varanasi", "India", 
"Rohtak", "Aurangabad", "Bellary", "Pondicherry", "Other City in Uttar Pradesh", 
"Nellore", "Karnal", "noida", "Panjim", "Anantapur", "Mohali", 
"Kozhikode", "Tuticorin", "Tirupati", "Kota", "Tirunelveli", 
"Meerut", "Kurukshetra", "Panipat", "Cuddalore", "Thrissur", 
"Ooty", "Haryana Other", "Chennai Bangalore", "Rajahmundry", 
"Amritsar", "Dubai", "gurgaon", "Warangal", "Kakinada", "UAE", 
"Rajkot", "Nagercoil"), N = c("9760", "7335", "5959", "4570", 
"4103", "3369", "2858", "2063", "1909", "1638", "1228", "543", 
"426", "404", "203", "189", "188", "176", "159", "150", "140", 
"136", "129", "125", "113", "112", "106", "101", "96", "84", 
"80", "80", "69", "67", "66", "63", "58", "55", "52", "50", "46", 
"45", "42", "41", "41", "40", "38", "33", "32", "32", "31", "30", 
"28", "27", "27", "26", "25", "24", "24", "23", "23", "21", "20", 
"20", "19", "16", "16", "16", "15", "15", "15", "15", "14", "14", 
"14", "13", "13", "13", "13", "12", "12", "12", "11", "11", "11", 
"11", "10", "10", "10", "10", "9", "9", "9", "9", "9", "9", "9", 
"9", "9", "9"), lon = c("77.5945627", "77.5945627", "72.8776559", 
"78.486671", "77.1024902", "80.2707184", "78.4982741", "77.3910265", 
"77.0266383", "121.0222565", "73.8567437", "88.363895", NA, "72.5713621", 
"73.0296625", "76.7794179", "77.4537578", "77.209031", "77.3177894", 
"72.8751786", "76.2673041", "76.9558321", NA, "83.2184815", "76.2673041", 
"76.2998842", "72.8310607", "80.946166", "77.5945627", "80.6480153", 
"85.8245398", "83.2184815", "76.9366376", "73.1812187", "75.8577258", 
"75.7138884", "74.8559568", "72.8776559", "79.0881546", "72.9780897", 
"75.7138884", "76.6393805", "77.412615", "91.7362365", "73.1812187", 
NA, "85.1375645", "78.7046725", "78.0321918", "81.6296413", NA, 
"75.8572758", "78.0080745", "73.7898023", "85.309562", "75.1239547", 
"85.8829895", "80.4365402", "77.1024902", "74.4976741", "76.085601", 
"76.7766974", "79.1324986", "78.1197754", "-70.8967155", "82.9739144", 
"78.96288", "76.606611", "75.3433139", "76.9214428", "79.8144722", 
"81.8306904", "79.986456", "76.9904825", "77.3910265", "73.8278496", 
"77.6005911", "76.7178726", "75.78041", "78.1348361", "79.4191795", 
"-103.2264256", "77.7566523", NA, "76.878282", "76.9635023", 
"79.7680243", "76.2144349", "76.6932438", "76.174513", "80.257028", 
"81.8040345", "74.8722642", "55.2707828", NA, "79.5940544", "82.2474648", 
"53.847818", "70.8021599", NA), lat = c("12.9715987", "12.9715987", 
"19.0759837", "17.385044", "28.7040592", "13.0826802", "17.4399295", 
"28.5355161", "28.4594965", "14.6090537", "18.5204303", "22.572646", 
NA, "23.022505", "19.0330488", "30.7333148", "28.6691565", "28.6139485", 
"28.4089123", "19.1538231", "9.9312328", "11.0168445", NA, "17.6868159", 
"9.9312328", "9.9816358", "21.1702401", "26.8466937", "12.9715987", 
"16.5061743", "20.2960587", "17.6868159", "8.5241391", "22.3071588", 
"22.7195687", "15.3172775", "12.9141417", "19.0759837", "21.1458004", 
"19.2183307", "19.7514798", "12.2958104", "23.2599333", "26.1445169", 
"22.3071588", NA, "25.5940947", "10.7904833", "30.3164945", "21.2513844", 
NA, "30.900965", "27.1766701", "19.9974533", "23.3440997", "15.3647083", 
"20.462521", "16.3066525", "28.7040592", "15.8496953", "29.0587757", 
"30.3781788", "12.9165167", "9.9252007", "42.51954", "25.3176452", 
"20.593684", "28.8955152", "19.8761653", "15.1393932", "11.9138598", 
"25.4484465", "14.4425987", "29.6856929", "28.5355161", "15.4909301", 
"14.6818877", "30.7046486", "11.2587531", "8.7641661", "13.6287557", 
"44.0801454", "8.7139126", NA, "29.9695121", "29.3909464", "11.744699", 
"10.5276416", "11.4064138", "28.409757", "13.004047", "17.0005383", 
"31.6339793", "25.2048493", NA, "17.9689008", "16.9890648", "23.424076", 
"22.3038945", NA)), .Names = c("m.i.location", "N", "lon", "lat"
), row.names = c(NA, -100L), class = c("data.table", "data.frame"
))

city2latlon$N<- as.numeric(city2latlon$N)
city2latlon$lat<- as.numeric(city2latlon$lat)
city2latlon$lon<- as.numeric(city2latlon$lon)
```


```{r}
# ### AchieveCodes 
## # that would save a .rds file for geocoded data, or geocode for lat long if it's the 1st time running
# ## making a list of file in current working directory
# filelist <- list.files()
# 
# # mydat exist, read it , otherwise, geocodes to find lat lon for the locations
# if(any(filelist=="mydat.rds") & exists("city2latlon")){
#   #read the created .rds containing the require data
#   mydat <- readRDS("mydat.rds")
# }else{
#   # using geocodes ( part of ggmap package) to find the lat and lon for the top 100 location
#   # perhaps not the cleanest way, some of the location will not be the most accurate.
#   geocodes <- geocode(as.character(mdt.melt.location.count.t100$m.i.location))
#   # might be better to batch feed it to avoid over query limit,
#   # which is an issue for network with good connection
#   city2latlon <- data.frame(mdt.melt.location.count.t100[,1:2],geocodes)
#   saveRDS(mydat, "mydat.rds")
# }
# ###


# India longitudes and latitude from google
india =  c(lon =78.96, lat = 20.59)

# #The locations that have absurd lon and lat are to be removed
# mydat.filtered <- setDT(mydat)[lon>60&lon<90] %>%
#   .[lat>5&lat<40]

# # loading the mapdata
# india.map = get_map(location = india, zoom = 5)
#
# # plotting with ggmap countour plot
# ggmap(india.map, extent = "panel", maprange=FALSE)+
#    geom_density2d(data = mydat.filtered, aes(x = lon, y = lat)) +
#    scale_fill_gradient(low = "green", high = "red") +
#    scale_alpha(range = c(0.00, 0.25), guide = FALSE) +
#    theme(legend.position = "none", axis.title = element_blank(), text = element_text(size = 12))
```


### Highchater: Hcmap

Via join with mapdata

```{r}
mapdata <- get_data_from_map(download_map_data("countries/in/in-all"))
# glimpse(mapdata)
# Observations: 34
# Variables: 20
# $ hc-group    <chr> "admin1", "admin1", "admin1", "admin1", "admin1", "admin1", "admin1", "adm...
# $ hc-middle-x <dbl> 0.65, 0.59, 0.50, 0.56, 0.46, 0.46, 0.51, 0.59, 0.47, 0.56, 0.60, 0.63, 0....
# $ hc-middle-y <dbl> 0.81, 0.63, 0.74, 0.38, 0.64, 0.51, 0.34, 0.41, 0.60, 0.32, 0.47, 0.55, 0....
# $ hc-key      <chr> "in-py", "in-ld", "in-wb", "in-or", "in-br", "in-sk", "in-ct", "in-tn", "i...
# $ hc-a2       <chr> "PY", "LD", "WB", "OR", "BR", "SK", "CT", "TN", "MP", "GU", "GA", "NL", "M...
# $ labelrank   <chr> "2", "2", "2", "2", "2", "2", "2", "2", "2", "2", "2", "2", "2", "2", "2",...
# $ hasc        <chr> "IN.PY", "IN.LD", "IN.WB", "IN.OR", "IN.BR", "IN.SK", "IN.CT", "IN.", "IN....
# $ alt-name    <chr> "Pondicherry|Puduchcheri|PondichÃ©ry", "Ã\u008dles Laquedives|Laccadive|Mi...
# $ woe-id      <chr> "20070459", "2345748", "2345761", "2345755", "2345742", "2345762", "200704...
# $ fips        <chr> "IN22", "IN14", "IN28", "IN21", "IN34", "IN29", "IN37", "IN22", "IN35", "I...
# $ postal-code <chr> "PY", "LD", "WB", "OR", "BR", "SK", "CT", "TN", "MP", NA, "GA", "NL", "MN"...
# $ name        <chr> "Puducherry", "Lakshadweep", "West Bengal", "Orissa", "Bihar", "Sikkim", "...
# $ country     <chr> "India", "India", "India", "India", "India", "India", "India", "India", "I...
# $ type-en     <chr> "Union Territory", "Union Territory", "State", "State", "State", "State", ...
# $ region      <chr> "South", "South", "East", "East", "East", "East", "Central", "South", "Cen...
# $ longitude   <chr> "79.7758", "72.7811", "87.7289", "84.4341", "85.8134", "88.4482", "82.3069...
# $ woe-name    <chr> "Puducherry", "Lakshadweep", "West Bengal", "Orissa", "Bihar", "Sikkim", "...
# $ latitude    <chr> "10.9224", "11.2249", "23.0523", "20.625", "25.6853", "27.5709", "21.8044"...
# $ woe-label   <chr> "Puducherry, IN, India", "Lakshadweep, IN, India", "West Bengal, IN, India...
# $ type        <chr> "Union Territor", "Union Territor", "State", "State", "State", "State", "S...
 
DT1 <- setDT(city2latlon)[, c("lon", "lat"), with = FALSE] %>%
  .[lon>60&lon<90] %>%
  .[lat>5&lat<30] %>%
  unique(., by = c('lon','lat')) 

DT2 <- setDT(mapdata)[, c("longitude", "latitude", "hc-a2"), with = FALSE] %>%
  .[, c("longitude","latitude"):=lapply(.SD, as.numeric), .SDcol=c("longitude","latitude")]
colnames(DT2) <- c("lon","lat", "hca2")

# checking the uniqueness
# DT1[,.N,c("lon","lat")][N>1L]
# DT2[,.N,c("longitude","latitude")][N>1L]

# Turns out joining on nearest value based on two column isn't trivial,  
# I have adapted a fuction from stackoverflow
# https://stackoverflow.com/questions/28435126/merge-data-table-by-two-nearest-variables?noredirect=1&lq=1
func = function(u,v)
{
    vec = with(DT2, (u-lon)^2 + (v-lat)^2)
    DT2[which.min(vec),]$hca2
}

t.join.data <- transform(DT1, 'hc-a2'=apply(DT1, 1, function(u) func(u[[1]], u[[2]]))) %>%
  .[city2latlon, on=.(lon,lat)] %>%
  .[!is.na('hc-a2'), value:=sum(N), by="hc-a2"] %>%
  unique(., by = c('hc-a2')) %>%
  .[lon>60&lon<90] %>%
  .[lat>5&lat<30]

# hcmap("countries/in/in-all", data = t.join.data, value = "value",
#       joinBy = c("hc-a2", "hc-a2"), name = "Number of job listing",
#       dataLabels = list(enabled = TRUE, format = '{point.name}'), borderWidth = 1,
#       tooltip = list(valueDecimals = 0, valuePrefix = "", valueSuffix = "")) 
```
Sadly I dont see an easy way to join the mapdata inforamtion together with the job listing information to provide state/region level information. i guess the likes of Mumbai, Bengaluru, Bangalore etc are actually cities name.

I resort to plotting them with the geocoded latitude and longitude with highchart, which depicted similar data to the ggmap in previous section.

```{r warning = F}
 # Setting z,color and name parameter for plotting with highcharter 
 highcharter.plot <- setDT(city2latlon)[lon>60&lon<90] %>%
   .[lat>5&lat<30]%>%
   .[,`:=`(
           name=m.i.location,
           z=N,
           color= colorize(N)
           )] 
 
# plotting with highcharter
hcmap("countries/in/in-all", name = "India") %>% 
  hc_add_series(data = highcharter.plot, type = "mapbubble",
                name = "Job listing", maxSize = "35") %>% 
    hc_title(text = "Locations of Job Posting") %>% 
  hc_subtitle(text = "only top 100 locations are included, the counts also include duplicates if the job is listed at multiple location") %>%
  hc_mapNavigation(enabled = TRUE) 

```


## IT Location plot
```{r}
# # melting for education , this would create duplicate job posting for difference level of education
# mdt.melt.location.Education <- melt(mdt.melt.location, 
#      measure.vars = patterns ("^m.Education.tf."), na.rm = T, variable.name = "EducationLevel") 
# mdt.melt.location.Education$value <- NULL 
# 
# mdt.melt.location.Education <- mdt.melt.location.Education %>%
#   .[!is.na(m.payrate.Mean)|industry=="IT-Software / Software Services",]
#   
# colnames(mdt.melt.location)

t.filtered.mdt<- mdt %>%
  .[!is.na(m.payrate.Mean)&industry=="IT-Software / Software Services"] 

# depiciting only the highest educational level per job listing
hcboxplot(x = t.filtered.mdt$m.payrate.Mean, 
          var = t.filtered.mdt$m.experience.Cut, 
          var2 = t.filtered.mdt$m.Education.factor, 
          outliers = FALSE) %>% 
  hc_chart(type = "column") %>%
  hc_title(text = "Plot of Payrate(Mean, PA) vs Experience (Yrs) segregated on Education")



## Highcharter plot to depict the location of job posting on map
highcharter.plot <- mdt.melt.location %>%
  .[!is.na(m.payrate.Mean)&industry=="IT-Software / Software Services",] %>%
  .[, .(.N, Paymedian=median(m.payrate.Mean), 
        Q1=quantile(m.payrate.Mean, 0.25),
        Q3=quantile(m.payrate.Mean, 0.75)), by = m.i.location] %>%
  .[order(-N)] %>%
  .[!is.na(m.i.location)] %>%
  .[mydat, on="m.i.location"]%>%
  .[lon>60&lon<90] %>%
  .[lat>5&lat<30] %>%
  .[,`:=`(
           name=m.i.location,
           z=N,
           color= colorize(Paymedian)
           )] 
  
## highcharter
hcmap("countries/in/in-all", name = "India") %>% 
  hc_add_series(data = highcharter.plot, type = "mapbubble",
                name = "Job listing", maxSize = "25") %>% 
    hc_title(text = "Locations of Job Posting for IT Software") %>% 
  hc_subtitle(text = "only top 100 locations are included, the calculation allowed multiple entries if the job is listed at multiple location") %>%
hc_tooltip(useHTML = TRUE,
             headerFormat = "<table>",
             pointFormat = paste("<tr><th colspan=\"1\">N Job Listing<td>{point.z}</td></th></tr>",
                                 "<tr><th>Location Name</th><td>{point.name} </td></tr>",
                                 "<tr><th>Pay Median</th><td>{point.Paymedian} </td></tr>",
                                 "<tr><th>Pay Q1</th><td>{point.Q1} </td></tr>",
                                 "<tr><th>Pay Q3</th><td>{point.Q3} </td></tr>"),
             footerFormat = "</table>")

```


## Healthcare location plot

```{r}
## Highcharter plot to depict the location of job posting on map
highcharter.plot <- mdt.melt.location %>%
  .[!is.na(m.payrate.Mean)&industry=="Medical / Healthcare / Hospitals",] %>%
  .[, .(.N, Paymedian=median(m.payrate.Mean), 
        Q1=quantile(m.payrate.Mean, 0.25),
        Q3=quantile(m.payrate.Mean, 0.75)), by = m.i.location] %>%
  .[order(-N)] %>%
  .[!is.na(m.i.location)] %>%
  .[mydat, on="m.i.location"]%>%
  .[lon>60&lon<90] %>%
  .[lat>5&lat<30] %>%
  .[,`:=`(
           name=m.i.location,
           z=N,
           color= colorize(Paymedian)
           )] 
  
# Highcharter
hcmap("countries/in/in-all", name = "India") %>% 
  hc_add_series(data = highcharter.plot, type = "mapbubble",
                name = "Job listing", maxSize = "25") %>% 
    hc_title(text = "Locations of Job Posting for Healthcare") %>% 
  hc_subtitle(text = "only top 100 locations are included, the calculation allowed multiple entries if the job is listed at multiple location") %>%
hc_tooltip(useHTML = TRUE,
             headerFormat = "<table>",
             pointFormat = paste("<tr><th colspan=\"1\">N Job Listing<td>{point.z}</td></th></tr>",
                                 "<tr><th>Location Name</th><td>{point.name} </td></tr>",
                                 "<tr><th>Pay Median</th><td>{point.Paymedian} </td></tr>",
                                 "<tr><th>Pay Q1</th><td>{point.Q1} </td></tr>",
                                 "<tr><th>Pay Q3</th><td>{point.Q3} </td></tr>"),
             footerFormat = "</table>")

```


##leaflet



```{r}
require(leaflet)
```

```{r}

software.icons <- awesomeIcons(
  icon = 'fa-github',
  library = 'fa'
)
internet.icons <- awesomeIcons(
  icon = 'fa-internet-explorer',
  library = 'fa'
)
bank.icon <- awesomeIcons(
  icon = 'fa-institution',
  library = 'fa'
)
factory.icon <- awesomeIcons(
  icon = '	fa-industry',
  library = 'fa'
)
recruit.icon <- awesomeIcons(
  icon = 'fa-handshake-o',
  library = 'fa'
)
hospital.icon <- awesomeIcons(
  icon = 'fa-h-square ',
  library = 'fa'
)
pharma.icon <- awesomeIcons(
  icon = 'fa-medkit',
  library = 'fa'
)
phone.icon <- awesomeIcons(
  icon = 'fa-phone',
  library = 'fa'
)

mydat<-setDT(mydat)[lon>60&lon<90] %>%
   .[lat>5&lat<30]

combined<- mdt.melt.location[,if(.N>300).SD, by=industry] %>%
  .[,if(.N>50).N, by=.(industry, m.i.location)] %>%
  .[mydat, on="m.i.location"] %>%
  .[lon>60&lon<90] %>%
  .[lat>5&lat<30]%>%
  droplevels(.) 

combined$industry



leaflet(combined) %>% addTiles() %>%
   addProviderTiles(providers$CartoDB.Positron)%>%
  addAwesomeMarkers(~lon+0.1, ~lat+0.1, icon=hospital.icon, label=~as.character(V1), 
                    data =combined[industry=="Medical / Healthcare / Hospitals"]) %>%
    addAwesomeMarkers(~lon+0.2, ~lat+0.2, icon=software.icons, label=~as.character(V1), 
                    data =combined[industry=="IT-Software / Software Services"]) %>%
  addAwesomeMarkers(~lon+0.3, ~lat+0.3, icon=phone.icon, label=~as.character(V1), 
                    data =combined[industry=="BPO / Call Centre / ITES"])  %>%
    addAwesomeMarkers(~lon, ~lat, icon=bank.icon, label=~as.character(V1),
                    data =combined[industry=="Strategy / Management Consulting Firms"]) %>%
  addAwesomeMarkers(~lon-0.1, ~lat-0.1, icon=recruit.icon, label=~as.character(V1),
                    data =combined[industry=="Recruitment / Staffing"])
  

pal <- colorFactor("viridis", domain = unique(combined$industry))

leaflet(combined) %>% addTiles() %>%  
  addProviderTiles(providers$CartoDB.Positron)%>%
  addCircleMarkers(lng = ~lon, lat = ~lat, weight = 10,
    radius = ~N*100,
    color = ~pal(industry),
    stroke = FALSE, fillOpacity = 0.5,
    clusterOptions = markerClusterOptions()
  )

leaflet(combined) %>% addTiles() %>%  
  addProviderTiles(providers$CartoDB.Positron)%>%
  addCircles(lng = ~lon, lat = ~lat, weight = 10,
    radius = ~N*100,
    color = ~pal(industry), 
    popup = ~m.i.location
  )


```


